{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHJIN STUDIOS: Box Office Success Blueprint\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "Ahjin Studios is taking a bold leap into the world of original video content. With major players in the industry producing record-breaking films, it's time we carve our own path to the silver screen. But before the cameras roll, we need to ground our creativity in strategy. This project explores which types of films are dominating the box office - genres, themes, release seasons, production budgets, and more - to identify what’s *actually working* in today’s film market.\n",
    "\n",
    "## Business Problem\n",
    "\n",
    "The entertainment industry is undergoing a massive transformation. Streaming giants and traditional studios alike are pouring billions into original content, resulting in a saturated, competitive, and fast-evolving market. Ahjin Studios, a newcomer in this arena, wants to make a strong, strategic entrance. But without prior experience in filmmaking or content production, the studio lacks a grounded understanding of what drives box office success.\n",
    "\n",
    "While creativity is the soul of cinema, data is its compass. The financial risk of producing a film is substantial, with production budgets often ranging from millions to hundreds of millions of dollars — and no guaranteed return. Choosing the wrong genre, misreading audience interests, or releasing at the wrong time can spell disaster. Conversely, aligning a film's concept with market demand can lead to runaway hits, brand recognition, and long-term profitability.\n",
    "\n",
    "Ahjin Studios needs clear, evidence-based insights to answer critical questions:\n",
    "\n",
    "- What types of movies are **worth betting on**?\n",
    "- Where can we find the **sweet spot between budget and revenue**?\n",
    "- Which trends are **passing fads**, and which are **sustainable opportunities**?\n",
    "- How can a **new studio** stand out in a market dominated by legacy franchises and big-name talent?\n",
    "\n",
    "By conducting a comprehensive analysis of recent box office performance, this project aims to **remove the guesswork** from movie production decisions and provide Ahjin Studios with a **strategic blueprint** for launching commercially viable, audience-ready films that can hold their own in today’s high-stakes entertainment landscape.\n",
    "\n",
    "## Objective\n",
    "\n",
    "To analyze recent box office trends and translate key findings into **actionable, data-driven recommendations** that will guide Ahjin Studios in developing high-performing original films.\n",
    "\n",
    "## Key Questions\n",
    "\n",
    "- Which **genres** are consistently earning the highest revenue?\n",
    "- What **budget range** yields the best ROI?\n",
    "- How do **release dates** affect performance?\n",
    "- Do **star power** and **director reputation** play a measurable role?\n",
    "- Are **franchise films** outperforming standalones?\n",
    "- What **audience demographics** are driving ticket sales?\n",
    "\n",
    "## Deliverables\n",
    "\n",
    "- A clean and exploratory dataset analysis of recent box office films\n",
    "- Visual breakdowns of top-performing genres, budgets, and seasons\n",
    "- A concise summary report with **strategic recommendations** for Ahjin Studios\n",
    "\n",
    "## Final Goal\n",
    "\n",
    "To provide the leadership team at Ahjin Studios with a **clear roadmap for movie production** - one that maximizes commercial success while carving out a unique space in the entertainment industry.\n",
    "\n",
    "> Lights, camera... data! Let's get to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INITIAL DATA EXPLORATION (IDE)\n",
    "\n",
    "Every dataset tells a story- but before we dive into any narratives, we'll flip through the table of contents. This phase is about getting comfortable with the data: seeing what’s there, what’s missing, and what might surprise us later if we don’t pay attention now.\n",
    "\n",
    "#### What's happening:\n",
    "- Importing key libraries like 'pandas', 'numpy', 'seaborn' and 'matplotlib'- the usual suspects for slicing, dicing and visualizing data and 'sql' for database manipulation.\n",
    "- Previewing the first few rows to get a feel for the dataset’s structure, naming conventions, and early red flags (no one likes nasty surprises 30 cells in).\n",
    "- Checking the shape of the data because whether it's 500 rows or 50,000 completely changes the game.\n",
    "- Get metadata\n",
    "- Get basic statistics information of both numerica and categorical columns\n",
    "\n",
    "This is where trust is built- between us and the dataset.\n",
    "\n",
    "Exploration done right is part instinct, part structure- this is BOTH!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical computation and data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SQLite3 module\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. BUDGET DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\Raw_Data\\\\tn.movie_budgets.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m budget_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mRaw_Data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtn.movie_budgets.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m budget_df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1706\u001b[0m     f,\n\u001b[0;32m   1707\u001b[0m     mode,\n\u001b[0;32m   1708\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1709\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1710\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1711\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1712\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1713\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1714\u001b[0m )\n\u001b[0;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    864\u001b[0m             handle,\n\u001b[0;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    866\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    867\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    869\u001b[0m         )\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '..\\\\Raw_Data\\\\tn.movie_budgets.csv'"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "budget_df = pd.read_csv(r'..\\Raw_Data\\tn.movie_budgets.csv', encoding = 'Latin1')\n",
    "budget_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape\n",
    "print(f\"The dataset has {budget_df.shape[0]} rows and {budget_df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names\n",
    "budget_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata\n",
    "budget_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display descriptive statistics for categorical columns\n",
    "budget_df.describe(include = 'O').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates and null values\n",
    "print(\"Duplicates:\", budget_df.duplicated().sum())\n",
    "print(\"\\nNull Values:\\n\", budget_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GROSS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gross dataset\n",
    "gross_df = pd.read_csv(r'..\\Raw_Data\\bom.movie_gross.csv')\n",
    "gross_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape \n",
    "print(f\"The dataset has {gross_df.shape[0]} rows and {gross_df.shape[1]} columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata \n",
    "gross_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic statistics for numerical columns\n",
    "gross_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic statistics for categorical columns\n",
    "gross_df.describe(include = 'O').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates and null values\n",
    "print(\"Duplicates:\", gross_df.duplicated().sum())\n",
    "print(\"\\nNull Values:\\n\", gross_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TMDB MOVIES DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TMDB movies dataset\n",
    "tmdb_df = pd.read_csv(r'..\\Raw_Data\\tmdb.movies.csv', index_col = 0)\n",
    "tmdb_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape\n",
    "print(f'The dataset has {tmdb_df.shape[0]} rows and {tmdb_df.shape[1]} columns.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names\n",
    "tmdb_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata\n",
    "tmdb_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display descriptive statistics for numerical columns\n",
    "tmdb_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display descriptive statistics for categorical columns\n",
    "tmdb_df.describe(include = 'O').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates and null values\n",
    "print('Duplicates:', tmdb_df.duplicated().sum())\n",
    "print('\\nNull Values:\\n', tmdb_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ROTTEN TOMATOES MOVIES INFORMATION DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "rt_movies_df = pd.read_csv(r\"..\\Raw_Data\\rt.movie_info.tsv\", sep = '\\t')\n",
    "rt_movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset shape\n",
    "print(f\"The dataset has {rt_movies_df.shape[0]} rows and {rt_movies_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display column names\n",
    "rt_movies_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata\n",
    "rt_movies_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic statistical information for categorical columns\n",
    "rt_movies_df.describe(include = 'O').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicates:\", rt_movies_df.duplicated().sum())\n",
    "print(\"\\nNull Values:\\n\", rt_movies_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. IMDB DATABASE EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database connection\n",
    "conn = sqlite3.connect(r'..\\Raw_Data\\im.db') \n",
    "cur = conn.cursor()\n",
    "\n",
    "# View all tables in the database\n",
    "cur.execute(\"\"\"SELECT name FROM sqlite_master WHERE type = 'table';\"\"\")\n",
    "cur.fetchall() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Focus Areas\n",
    "\n",
    "We’re working with a local IMDB database structured across multiple relational tables:\n",
    "\n",
    "- movie_basics \n",
    "- directors \n",
    "- known_for  \n",
    "- movie_akas  \n",
    "- movie_ratings  \n",
    "- persons\n",
    "- principals  \n",
    "- writers\n",
    "\n",
    "Our primarily focus is on the **movie_basics** and **movie_ratings** tables, as they form the foundation for exploring patterns in movie data and audience response.\n",
    "\n",
    "At this stage, we’ll begin by:\n",
    "- Inspecting the structure and columns of each table\n",
    "- Understanding how the tables relate (e.g. shared keys)\n",
    "- Identifying relevant variables for our analysis\n",
    "\n",
    "Our goal is to uncover trends, correlations, and insights from movie metadata (like genre, title, year). Let’s start by peeking inside the structure of these two key tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Basics Table Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to movie_basics table and print out first few rows\n",
    "moviebasics_df = pd.read_sql(\n",
    "    \"\"\"\n",
    "        SELECT * \n",
    "        FROM movie_basics\n",
    "    \"\"\", conn)\n",
    "\n",
    "moviebasics_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table shape\n",
    "print(f\"The table contains {moviebasics_df.shape[0]} rows and {moviebasics_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get column names\n",
    "moviebasics_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata\n",
    "moviebasics_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic statistical information for all categorical variables\n",
    "moviebasics_df.describe(include = 'O').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic statistical information for all numeric variables\n",
    "moviebasics_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of duplicates and null values\n",
    "print(\"Duplicates:\", moviebasics_df.duplicated().sum())\n",
    "print(\"\\nNull Values:\\n\", moviebasics_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie Ratings Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load movie_ratings table \n",
    "movieratings_df = pd.read_sql(\n",
    "    \"\"\"\n",
    "        SELECT * \n",
    "        FROM movie_ratings\n",
    "    \"\"\", conn)\n",
    "\n",
    "movieratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table shape\n",
    "print(f\"The table contains {movieratings_df.shape[0]} rows and {movieratings_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# F+Get column names\n",
    "movieratings_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata\n",
    "movieratings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic statistical information of numeric variables\n",
    "movieratings_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find out number of duplicates and null values present\n",
    "print(\"Duplicates:\", movieratings_df.duplicated().sum())\n",
    "print(\"\\nNull Values:\\n\", movieratings_df.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two datasets on the common column movie_id\n",
    "q = \"\"\"\n",
    "    SELECT \n",
    "        mb.movie_id,\n",
    "        mb.primary_title,\n",
    "        mb.original_title,\n",
    "        mb.start_year,\n",
    "        mb.runtime_minutes,\n",
    "        mb.genres,\n",
    "        mr.averagerating,\n",
    "        mr.numvotes\n",
    "    FROM movie_basics mb\n",
    "    INNER JOIN movie_ratings mr ON mb.movie_id = mr.movie_id;\n",
    "\"\"\"\n",
    "\n",
    "movies_df = pd.read_sql(q, conn)\n",
    "movies_df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get new dataset shape\n",
    "print(f\"The dataset contains {movies_df.shape[0]} rows and {movies_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column names\n",
    "movies_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metadata\n",
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic statistical information for numeric columns\n",
    "movies_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic statistical information for categorical columns\n",
    "movies_df.describe(include = 'O').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get duplicate and null values counts\n",
    "print(\"Duplicates:\", movies_df.duplicated().sum())\n",
    "print(\"\\nNull Values:\\n\", movies_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBSERVATIONS AND EARLY INSIGHTS\n",
    "\n",
    "We conducted an Initial Data Exploration (IDE) across the five key datasets powering our movie analytics project. The goal was to understand data structure, completeness, and quality before diving into cleaning or storytelling. Below is a summary of our early findings:\n",
    "\n",
    "### 1. **Movie Budgets Dataset**\n",
    "- **Shape**: 5,782 rows × 6 columns  \n",
    "- **Key Columns**: release_date, movie, production_budget, domestic_gross, worldwide_gross\n",
    "- **Highlights**:\n",
    "  - No missing values — dataset is fully complete.\n",
    "  - All monetary fields are stored as strings (e.g., \"$100,000,000\") and require conversion to numeric types.\n",
    "  - Over 500 movies show $0 in domestic_gross, and 367 in worldwide_gross. These likely indicate unreported or unavailable data.\n",
    "  - No duplicate entries found.\n",
    "\n",
    "### 2. **Gross Revenue Dataset**\n",
    "- **Shape**: 3,387 rows × 5 columns  \n",
    "- **Key Columns**: title, studio, domestic_gross, foreign_gross, year\n",
    "- **Highlights**:\n",
    "  - foreign_gross has 1,350 missing values — over one-third of the dataset.\n",
    "  - Minor missingness in studio (5 rows) and domestic_gross (28 rows).\n",
    "  - Monetary columns are stored as strings — will need cleaning for analysis.\n",
    "  - No duplicate rows detected.\n",
    "\n",
    "### 3. **TMDB Movies Dataset**\n",
    "- **Shape**: 26,517 rows × 9 columns  \n",
    "- **Key Columns**: genre_ids, original_title, popularity, vote_average, vote_count\n",
    "- **Highlights**:\n",
    "  - No missing values — data is complete.\n",
    "  - 1,020 duplicate entries found; will need deduplication.\n",
    "  - genre_ids are stored as stringified lists — need to be parsed for genre-level insights.\n",
    "  - Rich data on audience sentiment, popularity trends, and voting behavior.\n",
    "\n",
    "### 4. **IMDb Database (Merged: movie_basics + movie_ratings)**\n",
    "- **Shape**: 73,856 rows × 8 columns  \n",
    "- **Key Columns**: primary_title, original_title, start_year, runtime_minutes, genres, averagerating, numvotes\n",
    "- **Highlights**:\n",
    "  - runtime_minutes has 7,620 missing entries (~10% of data).\n",
    "  - genres missing in 804 rows.\n",
    "  - One extreme outlier in runtime_minutes (~51,420 mins) — needs capping or removal.\n",
    "  - No duplicate records.\n",
    "  - Ratings data is robust — average rating is ~6.3, normally distributed around 6.5.\n",
    "  - Voting counts vary drastically, with some titles exceeding 1.8M votes.\n",
    "  - Documentary is the most common genre.\n",
    "\n",
    "These insights help prioritize cleaning tasks and shape our storytelling strategy. We'll focus on handling missingness (especially runtimes and revenues), converting key fields to numeric types, and isolating outliers before diving into visual storytelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA WRANGLING\n",
    "\n",
    "To prepare the datasets for analysis, we perform a series of data cleaning and transformation steps across all sources:\n",
    "\n",
    "### Universal Cleaning Tasks\n",
    "- **Convert currency strings to numeric** in all budget and gross-related columns (e.g production_budget, domestic_gross, foreign_gross, etc.).\n",
    "- **Handle missing values**, particularly in:\n",
    "  - foreign_gross (Gross Revenue dataset)\n",
    "  - box_office and review fields (Rotten Tomatoes dataset)\n",
    "  - runtime_minutes and genres (IMDB)\n",
    "- **Drop duplicate entries**:\n",
    "  - TMDB Movies: 1,020 duplicates\n",
    "\n",
    "### Dataset-Specific Wrangling\n",
    "\n",
    "#### Dates & Titles\n",
    "- **Standardize movie titles and release dates** across all datasets to improve matchability.\n",
    "- Parse and convert date fields (e.g release_date, year, start_year) into consistent datetime formats.\n",
    "\n",
    "#### IMDB Tables\n",
    "- **Convert outliers and handle anomalies**:\n",
    "  - Address unrealistic values in runtime_minutes (e.g 51,420 minutes)\n",
    "- **Parse and standardize genre strings** from pipe-delimited format to lists or categories for analysis.\n",
    "- Ensure movie_id is consistently typed (object/string) across movie_basics and movie_ratings before merging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HANDLING NULL VALUES\n",
    "#### 1. GROSS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = gross_df.isna().sum()\n",
    "null_percentages = (null_counts / len(gross_df)) * 100\n",
    "\n",
    "print(\"Missing Values Overview:\\n\")\n",
    "print(pd.concat([null_counts.rename(\"Null Count\"), null_percentages.round(2).rename(\"Null Percentage (%)\")], axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'studio' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rows with nulls\n",
    "gross_df.loc[gross_df['studio'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute nulls in studio column\n",
    "gross_df.loc[gross_df['title'] == 'Plot for Peace', 'studio'] = 'Trinity Films'\n",
    "gross_df.loc[gross_df['title'] == 'Fireflies in the Garden', 'studio'] = 'Senator Entertainmet Inc.'\n",
    "gross_df.loc[gross_df['title'] == 'Keith Lemon: The Film', 'studio'] = 'Lionsgate'\n",
    "gross_df.loc[gross_df['title'] == 'Secret Superstar', 'studio'] = 'Zee Studios'\n",
    "gross_df.loc[gross_df['title'] == 'Outside the Law (Hors-la-loi)', 'studio'] = 'StudioCanal'\n",
    "\n",
    "# Preview changes\n",
    "print(\"Null values after imputation:\", gross_df['studio'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'domestic_gross' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rows with nulls\n",
    "gross_df.loc[gross_df['domestic_gross'].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_gross = gross_df['domestic_gross'].median()\n",
    "gross_df['domestic_gross'] = gross_df['domestic_gross'].fillna(median_gross)\n",
    "\n",
    "print(\"Null Values after imputation:\", gross_df['domestic_gross'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize = (14, 5))\n",
    "\n",
    "# Boxplot\n",
    "sns.boxplot(data=gross_df, x='domestic_gross', ax = axes[0], color = 'skyblue')\n",
    "axes[0].set_title(\"Boxplot of Domestic Gross\")\n",
    "axes[0].set_xlabel(\"Domestic Gross\")\n",
    "\n",
    "# Histogram with KDE\n",
    "sns.histplot(data = gross_df, x = 'domestic_gross', bins=50, kde=True, ax = axes[1], color = 'lightgreen')\n",
    "axes[1].set_title(\"Histogram of Domestic Gross\")\n",
    "axes[1].set_xlabel(\"Domestic Gross Bins\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'foreign_gross' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview rows with nulls in foreign_gross\n",
    "gross_df.loc[gross_df['foreign_gross'].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique values in foreign_gross\n",
    "gross_df.loc[~gross_df['foreign_gross'].astype(str).str.replace(',', '').str.isnumeric(), 'foreign_gross'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation strategy:\n",
    "\n",
    "# Coerce to numeric\n",
    "gross_df['foreign_gross'] = pd.to_numeric(gross_df['foreign_gross'], errors='coerce')\n",
    "\n",
    "# Impute median\n",
    "median_foreign_gross = gross_df['foreign_gross'].median()\n",
    "gross_df['foreign_gross'] = gross_df['foreign_gross'].fillna(median_foreign_gross)\n",
    "\n",
    "# Preview changes\n",
    "print(\"Null Values after imputation:\", gross_df['foreign_gross'].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize = (14, 5))\n",
    "\n",
    "# Boxplot\n",
    "sns.boxplot(data=gross_df, x='foreign_gross', ax = axes[0], color = 'salmon')\n",
    "axes[0].set_title(\"Boxplot of Foreign Gross\")\n",
    "axes[0].set_xlabel(\"Foreign Gross\")\n",
    "\n",
    "# Histogram with KDE\n",
    "sns.histplot(data = gross_df, x = 'foreign_gross', bins = 50, kde = True, ax = axes[1], color = 'lightblue')\n",
    "axes[1].set_title(\"Histogram of Foreign Gross\")\n",
    "axes[1].set_xlabel(\"Foreign Gross Bins\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OTLIER HANDLING IN GROSS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(gross_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comment on Outliers\n",
    "\n",
    "> The boxplots for both **domestic** and **foreign gross** clearly reveal the presence of extreme outliers - films that have earned substantially more than the majority. In the movie industry, these are typically blockbuster titles, franchise installments, or globally viral releases with massive marketing budgets and wide theatrical distribution. These high-grossing films, while statistically extreme, represent genuine and meaningful data points.\n",
    ">\n",
    "> **Removing these outliers would strip away the very success stories we aim to emulate**. Therefore, we will **retain the outliers** in our analysis to ensure we capture the full spectrum of market performance and derive actionable insights from the most profitable examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROTTEN TOMATOES MOVIE INFORMATION DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_counts = rt_movies_df.isna().sum()\n",
    "null_percentages = (null_counts / len(rt_movies_df)) * 100\n",
    "\n",
    "print(\"Missing Values Overview:\\n\")\n",
    "print(pd.concat([null_counts.rename(\"Null Count\"), null_percentages.round(2).rename(\"Null Percentage (%)\")], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_movies_df.loc[rt_movies_df['genre'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_movies_df.loc[rt_movies_df['rating'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_movies_df.loc[rt_movies_df['director'].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop useless columns\n",
    "rt_movies_df = rt_movies_df.drop(columns = [\n",
    "    \"studio\",\n",
    "    \"dvd_date\",\n",
    "    \"currency\",\n",
    "    \"synopsis\",\n",
    "])\n",
    "\n",
    "rt_movies_df = rt_movies_df.rename(columns = {\"rating\": \"maturity_rating\"})\n",
    "\n",
    "# Drop useless rows\n",
    "rt_movies_df = rt_movies_df.dropna(subset = [\"maturity_rating\", \"genre\"])\n",
    "\n",
    "# Fill remaining categorical columns with 'unknows'\n",
    "cat_cols = rt_movies_df.select_dtypes(include = ['object']).columns\n",
    "\n",
    "for col in cat_cols:\n",
    "   rt_movies_df[col].fillna('Unknown', inplace = True)\n",
    "\n",
    "# Preview changes\n",
    "rt_movies_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MERGED IMDB DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get null values percentage to guide on imputation\n",
    "null_counts = movies_df.isna().sum()\n",
    "null_percentages = (null_counts / len(movies_df)) * 100\n",
    "\n",
    "print(\"Missing Values Overview:\\n\")\n",
    "print(pd.concat([null_counts.rename(\"Null Count\"), null_percentages.round(2).rename(\"Null Percentage (%)\")], axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values in runtime_minutes with median because it is more statistically sound\n",
    "median_runtime = movies_df['runtime_minutes'].median()\n",
    "movies_df['runtime_minutes'] = movies_df['runtime_minutes'].fillna(median_runtime)\n",
    "\n",
    "# Drop null values in 'genres'\n",
    "movies_df = movies_df.dropna(subset = ['genres'])\n",
    "\n",
    "# Preview changes\n",
    "print(\"Missing Values after imputation:\\n\", movies_df.isna().sum())\n",
    "\n",
    "# Check new shape\n",
    "print(f\"\\nThe dataset now contains {movies_df.shape[0]} rows and {movies_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OUTLIER HANDLING FOR MERGED IMDB DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(movies_df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Commentary\n",
    "\n",
    "The boxplot reveals clear outliers across multiple numeric columns:\n",
    "\n",
    "- **runtime_minutes**: Several films significantly exceed the typical range, likely due to special formats (e.g director’s cuts, miniseries logged as films) or data entry errors. These extreme values are sparse but can skew distribution-based analyses like mean runtime or standard deviation.\n",
    "\n",
    "- **numvotes**: This variable exhibits extreme right-skewness, with a small subset of movies receiving disproportionately high votes - likely blockbusters or cult classics. These outliers are *genuine signals*, not noise, and provide valuable insight into popularity dynamics. However, log-transforming or binning may be helpful if modeling or clustering is later considered.\n",
    "\n",
    "- **averagerating** and **start_year**: Outliers here are less severe. A few ratings may hit extreme values (e.g unusually low or high), but IMDb caps the rating scale, so distortion is limited. For start_year, occasional anomalies (e.g pre-1900 entries or far-future entries) may be metadata quirks or special historical/fictional content.\n",
    "\n",
    "**Takeaway**: Most outliers are *contextual* - they often carry meaning rather than being simple anomalies. Thus, should not be dropped blindly. Instead, they offer rich angles for narrative exploration, especially when highlighting patterns in niche genres, cult films, or legacy cinema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HANDLING DUPLICATES \n",
    "#### 1. TMDB MOVIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicates before imputation:\", tmdb_df.duplicated().sum())\n",
    "\n",
    "tmdb_df = tmdb_df.drop_duplicates()\n",
    "print(\"\\nDuplicates after imputation:\", tmdb_df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HANDLING CATEGORICAL VARIABLES\n",
    "\n",
    "#### BUDGET DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_clean = ['production_budget', 'domestic_gross', 'worldwide_gross']\n",
    "\n",
    "for col in cols_to_clean:\n",
    "    budget_df[col] = (\n",
    "        budget_df[col]\n",
    "        .str.replace(r'[\\$,]', '', regex = True) \n",
    "        .astype(float)                          \n",
    "    )\n",
    "\n",
    "budget_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GROSS DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gross_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXPLORATORY DATA ANALYSIS (EDA)\n",
    "\n",
    "This section is dedicated to uncovering the underlying patterns, structures, and quirks within our movie datasets. By dissecting the data through visual and statistical lenses, we aim to understand how key features behave individually and in relation to one another.\n",
    "\n",
    "We’ll kick things off with a time series analysis to explore how movie trends have evolved over time. After that, we’ll move into univariate(single-variable)distributions, bivariate (two-variable) relationships, and multivariate (multiple-variable) interactions to reveal deeper insights and correlations.\n",
    "\n",
    "The objective here is clarity: what’s happening in the data, when it happens, and why it might matter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. TIME SERIES ANALYSIS\n",
    "Movies live and die by release timing - trends shift, markets evolve, and box office behavior often hinges on when a film hits theaters. To tap into this temporal dynamic, we begin by converting all relevant date columns to proper datetime format. This enables powerful time-based operations: resampling, trend detection, seasonal decomposition, and more.\n",
    "\n",
    "In this section, we:\n",
    "\n",
    "- Convert all date-related columns to datetime objects\n",
    "\n",
    "- Inspect temporal coverage and granularity\n",
    "\n",
    "- Visualize key metrics over time (e.g number of movies released, budget trends, gross revenue dynamics)\n",
    "\n",
    "- Identify long-term patterns, spikes, or declines tied to specific periods (e.g pre/post-pandemic, summer releases)\n",
    "\n",
    "- Let’s bring time into the frame - because in film, timing is everything."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. BUDGET DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_df['release_date'] = pd.to_datetime(budget_df['release_date'], errors = 'coerce')\n",
    "\n",
    "# Extract relevant date components\n",
    "budget_df['release_year'] = budget_df['release_date'].dt.year\n",
    "budget_df['release_month'] = budget_df['release_date'].dt.month\n",
    "budget_df['release_weekday'] = budget_df['release_date'].dt.day_name()\n",
    "budget_df['release_quarter'] = budget_df['release_date'].dt.quarter\n",
    "\n",
    "budget_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. UNIVARIATE ANALYSIS\n",
    "\n",
    "Univariate analysis helps us understand the distribution, central tendency, and spread of individual variables in isolation. In this section, we’ll explore key numerical and categorical features, such as production budgets, revenues, genres, and runtimes, to detect skewness, outliers, and potential data quality issues.\n",
    "\n",
    "This step lays the foundation for more complex relationships by first mastering each variable on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bins and labels\n",
    "bins = [0, 25_000_000, 50_000_000, 75_000_000, 100_000_000, 150_000_000, 200_000_000]\n",
    "labels = ['<25M', '25–50M', '50–75M', '75–100M', '100–150M', '150–200M']\n",
    "\n",
    "# Create bin column\n",
    "budget_df['budget_bin'] = pd.cut(budget_df['production_budget'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Count per bin\n",
    "bin_counts = budget_df['budget_bin'].value_counts().sort_index()\n",
    "print(bin_counts)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))  # ✅ Correct way to set size\n",
    "bin_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Production Budget Bins')\n",
    "plt.xlabel('Budget Range')\n",
    "plt.ylabel('Number of Films')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean (if not already done)\n",
    "budget_df['worldwide_gross'] = budget_df['worldwide_gross'].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# Adjusted bins based on range\n",
    "bins = [0, 50_000_000, 100_000_000, 200_000_000, 500_000_000, 1_000_000_000, 2_000_000_000]\n",
    "labels = ['<50M', '50–100M', '100–200M', '200–500M', '500M–1B', '>1B']\n",
    "\n",
    "# Binning\n",
    "budget_df['worldwide_bin'] = pd.cut(budget_df['worldwide_gross'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Count and plot\n",
    "bin_counts = budget_df['worldwide_bin'].value_counts().sort_index()\n",
    "print(bin_counts)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bin_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Worldwide Gross Bins')\n",
    "plt.xlabel('Worldwide Gross Range')\n",
    "plt.ylabel('Number of Films')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with annotations\n",
    "ax = bin_counts.plot(kind='bar', color='skyblue', edgecolor='black', figsize=(10,6))\n",
    "plt.title('Domestic Gross Bins')\n",
    "plt.xlabel('Domestic Gross Range ($)')\n",
    "plt.ylabel('Number of Films')\n",
    "\n",
    "# Add count labels above bars\n",
    "for i, count in enumerate(bin_counts):\n",
    "    ax.text(i, count + 1, str(count), ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genre Distribution Analysis\n",
    "# First, let's split the pipe-delimited genres and count individual genres\n",
    "all_genres = []\n",
    "for genre_string in movies_df['genres'].dropna():\n",
    "    genres = genre_string.split('|')\n",
    "    all_genres.extend(genres)\n",
    "\n",
    "# Create a DataFrame for genre counts\n",
    "genre_counts = pd.Series(all_genres).value_counts().head(15)\n",
    "print(\"Top 15 Genres by Number of Films:\")\n",
    "print(genre_counts)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "genre_counts.plot(kind='barh', color='lightgreen', alpha=0.7)\n",
    "plt.title('Top 15 Most Common Movie Genres')\n",
    "plt.xlabel('Number of Films')\n",
    "plt.ylabel('Genre')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB Average Ratings Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(movies_df['averagerating'], bins=30, alpha=0.7, color='gold', edgecolor='black')\n",
    "plt.title('Distribution of IMDB Average Ratings')\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"IMDB Average Ratings Summary:\")\n",
    "print(movies_df['averagerating'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maturity Rating Distribution (from Rotten Tomatoes data)\n",
    "plt.figure(figsize=(10, 6))\n",
    "rating_counts = rt_movies_df['maturity_rating'].value_counts()\n",
    "rating_counts.plot(kind='bar', color='orange', alpha=0.7)\n",
    "plt.title('Distribution of Movie Maturity Ratings')\n",
    "plt.xlabel('Maturity Rating')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"Maturity Rating Distribution:\")\n",
    "print(rating_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Studio Distribution (from Gross data)\n",
    "plt.figure(figsize=(12, 8))\n",
    "studio_counts = gross_df['studio'].value_counts().head(15)\n",
    "studio_counts.plot(kind='barh', color='lightcoral')\n",
    "plt.title('Top 15 Movie Studios by Number of Films')\n",
    "plt.xlabel('Number of Films')\n",
    "plt.ylabel('Studio')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"Top 10 Studios:\")\n",
    "print(studio_counts.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release Year Distribution (from IMDB data)\n",
    "plt.figure(figsize=(14, 6))\n",
    "year_counts = movies_df['start_year'].value_counts().sort_index()\n",
    "\n",
    "# Filter to recent years for better visualization\n",
    "recent_years = year_counts[year_counts.index >= 2000]\n",
    "recent_years.plot(kind='line', marker='o', color='navy', linewidth=2, markersize=4)\n",
    "plt.title('Number of Movies Released by Year (2000 onwards)')\n",
    "plt.xlabel('Release Year')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"Movies by Recent Years (2015-2020):\")\n",
    "print(recent_years.tail(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIVARIATE ANALYSIS\n",
    "#### 1.WHICH MOVIE GENRES ARE CONSISTENTLY EARNING THE HIGHEST ROI? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Worked with the gross_df and compared budget for production with gross income\n",
    "budget_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROI for each movie\n",
    "budget_df['roi'] = (budget_df['worldwide_gross'] - budget_df['production_budget']) /budget_df['production_budget']\n",
    "budget_df['roi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm new column\n",
    "budget_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot them\n",
    "top_movies = budget_df.sort_values('roi', ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.bar(top_movies['movie'], top_movies['roi'], color='skyblue')\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "plt.ylabel('ROI')\n",
    "plt.title('Top 10 Movies by ROI')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBSERVATIONS\n",
    "#### . 'Deep Throat' has surpassed all other movies with the ROI,second highest being 'Mad max'\n",
    "#### . Movies from 'Paranormal Activity' down to 'The Night of the living dead'have relatively similar ROI's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. HOW DO RELEASE DATES AFFECT PERFORMANCE?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert theater dates and handle errors\n",
    "rt_movies_df['theater_date'] = pd.to_datetime(rt_movies_df['theater_date'], errors='coerce')\n",
    "\n",
    "# 2. Clean box office data - handle 'Unknown' and empty strings\n",
    "rt_movies_df['box_office'] = (\n",
    "    rt_movies_df['box_office']\n",
    "    .replace('Unknown', pd.NA)  \n",
    "    .astype(str) \n",
    "    .str.replace(r'[^\\d.]', '', regex=True)  \n",
    "    .replace('', pd.NA))\n",
    "\n",
    "# 3. Convert to numeric and drop NA values\n",
    "rt_movies_df['box_office'] = pd.to_numeric(rt_movies_df['box_office'], errors='coerce')\n",
    "plot_data = rt_movies_df.dropna(subset=['theater_date', 'box_office']).copy()\n",
    "\n",
    "# 4. Create the plot with proper filtering\n",
    "plt.figure(figsize=(14, 7))\n",
    "\n",
    "# 5. Bar plot by month\n",
    "plot_data['month'] = plot_data['theater_date'].dt.month\n",
    "monthly_avg = plot_data.groupby('month')['box_office'].mean()\n",
    "\n",
    "sns.barplot(x=monthly_avg.index, y=monthly_avg.values, palette='coolwarm')\n",
    "plt.title('Average Box Office by Release Month (Excluding Missing Data)', fontsize=14)\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Average Revenue ($)')\n",
    "plt.xticks(ticks=range(12), labels=['Jan','Feb','Mar','Apr','May','Jun',\n",
    "                                  'Jul','Aug','Sep','Oct','Nov','Dec'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBSERVATIONS\n",
    "#### . Shows highest ROI is during holiday seasons (around November - December), with december showing the highest income rate\n",
    "#### . significant peaks during summer(around June - August)\n",
    "#### . Lowest performance is in early spring and late fall i.e March,April,October,November\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. DO LONGER OR SHORTER MOVIES GENERATE MORE OR LESS INCOME?\n",
    "\n",
    "#### So we plotted box_office against Runtime to analyze how movie runtime affects financial performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert runtime to numeric minutes\\\n",
    "df = rt_movies_df.copy()\n",
    "df['runtime_min'] = df['runtime'].str.extract('(\\d+)').astype(float)\n",
    "\n",
    "# Clean box office data\n",
    "def clean_box_office(value):\n",
    "    if pd.isna(value) or value == 'Unknown':\n",
    "        return None\n",
    "    value_str = str(value).strip()\n",
    "    cleaned = ''.join(c for c in value_str if c.isdigit() or c == '.')\n",
    "    return float(cleaned) if cleaned else None\n",
    "\n",
    "# Apply the cleaning function\n",
    "rt_movies_df['box_office'] = rt_movies_df['box_office'].apply(clean_box_office)\n",
    "\n",
    "# Set up runtime categories\n",
    "bins = [0, 90, 105, 120, 150, 200, 300]\n",
    "labels = ['<90m', '90-105m', '105-120m', \n",
    "               '120-150m', '150-200m', '200m+']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "ax = sns.barplot(data=df.dropna(),\n",
    "                x=pd.cut(df['runtime_min'], bins=bins, labels=labels),\n",
    "                y='box_office',\n",
    "                estimator=np.median,\n",
    "                errorbar=None,\n",
    "                color='dodgerblue')\n",
    "\n",
    "plt.title('Average Box office rates by Duration of movies', pad=15)\n",
    "plt.xlabel('Runtime')\n",
    "plt.ylabel('Revenue ($)')\n",
    "plt.grid(axis='y', alpha=0.2)\n",
    "\n",
    "# Add value labels\n",
    "for bar in ax.containers[0]:\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "            f'${bar.get_height():,.0f}',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBSERVATIONS\n",
    "#### . Movies with runtimes between 150-200 minutes have the highest average box office revue\n",
    "#### . On the other hand, movies with more than 200 minutes have the least box office revenue\n",
    "#### . The average revenue increases significantlly for categories between <90 min up to 150-200 min, showing a positive correlation of the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. WHAT IS THE DISTRIBUTION OF FILMS RELEASED BY EACH STUDIO?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Studio Distribution (from Gross data) against number of films\n",
    "plt.figure(figsize=(12, 8))\n",
    "studio_counts = gross_df['studio'].value_counts().head(15)\n",
    "studio_counts.plot(kind='barh', color='lightcoral')\n",
    "plt.title('Top 15 Movie Studios by Number of Films')\n",
    "plt.xlabel('Number of Films')\n",
    "plt.ylabel('Studio')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"Top 10 Studios:\")\n",
    "print(studio_counts.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBSERVATIONS\n",
    "#### . 'IFC' studios has the highest number of films, with 'UNI' & 'WB' coming in close\n",
    "#### . 'FOXs' studios has the least number of films"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. HOW DOES MOVIE BUDGET VARY OVER THE YEARS?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert release_date to date time\n",
    "budget_df['release_date'] = pd.to_datetime(budget_df['release_date'], errors='coerce')\n",
    "\n",
    "# extract the years\n",
    "budget_df['release_year'] = budget_df['release_date'].dt.year\n",
    "budget_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert infinite values to NAN\n",
    "budget_df['production_budget'] = budget_df['production_budget'].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# plot production budget vs release year\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=budget_df, x='release_year', y='production_budget')\n",
    "\n",
    "plt.title('Production Budget over Release Year', pad=15)\n",
    "plt.xlabel('Release Year')\n",
    "plt.ylabel('Production Budget ($)')\n",
    "plt.grid(True, axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBSERVATIONS\n",
    "#### . The average production budget remains very low and relatively stable in the early years(1910s-1950s)\n",
    "#### . Starting around 1960s, there's an upward trend that continues to the 1970s and 80s \n",
    "#### . There's an accelerated growth from 1990s peaking to 2020\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
